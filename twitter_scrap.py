# -*- coding: utf-8 -*-
"""twitter_scrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aeCpKBLpmmThNzIKevDmoRi7SQoxp7e0
"""

!pip install snscrape
!pip install streamlit
!pip install jedi
!pip install -q streamlit
!pip install pymongo

from google.colab import files
from PIL import Image
from io import BytesIO
uploaded = files.upload()
im = Image.open(BytesIO(uploaded['twitter_logo.jpg']))

#%%writefile streamlit.py
import snscrape.modules.twitter as sntwitter
import pymongo
import datetime
import streamlit as st
import pandas as pd
import json
import base64
from google.colab import files
from PIL import Image
from io import BytesIO
import plotly.express as px

twitter_banner = Image.open('/content/twitter_logo.jpg')
st.image(twitter_banner)

#title of api
st.title('Twitter scraping')
st.write('Twitter scraping API allows developers to extract data from Twitter, such as tweets, followers, and user profiles. This data can be used for various purposes, such as sentiment analysis, social media monitoring, and market research.')

padding = 0
with st.sidebar:
 #with st.container():
#user input search query from user
  search_query = st.text_input('Enter your search query: ')

#user input starting data 
  start_date = st.date_input('select a date in YYYY-MM-DD format: ')

#user input end date
  end_date = st.date_input('select a date in YYYY-MM-DD format: ', key='end_date')

#slider to limit the scrape
  tweet_limit = st.slider('Enter the number of tweets you want to scrape: ', 0, 5000)

#enabling start button to search query 
  start= st.button("Start Scrape")

#def func for start scrapping the required data
  def tweet_scrap(search_query,start_date,end_date,tweet_limit):
    tweet_list=[]
    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search_query} since:{start_date} until:{end_date} filter:links -filter:replies').get_items()):
       data={
            'keyword': search_query,
            'date': tweet.date,
            'id': tweet.id,
            'url': tweet.url,
            'user': tweet.user.username,
            'content': tweet.content,
            'replies': tweet.replyCount,
            'likes': tweet.likeCount,
            'retweets': tweet.retweetCount,
            'lang': tweet.lang,
            'source': tweet.source
          }    
       tweet_list.append(data)
       if i==tweet_limit - 1:
         break
  
    return tweet_list

#def func for data frame creation
  def display_df(tweet_list):
    df = pd.DataFrame(tweet_list, columns=["keyword","date","id","url","user","content","replies","likes","retweets","lang","source"])
    return df

if start:
     start = tweet_scrap(search_query,start_date,end_date,tweet_limit)
     mongo_data = display_df(start)
     data_frame = display_df(start).head()
     st.success("Here's your sample data : ")
     st.dataframe(data_frame)
#MongoDB connection and database collection setup
     client = pymongo.MongoClient("mongodb+srv://aravindh1895:18Alpacino@cluster0.xmbwreu.mongodb.net/?retryWrites=true&w=majority")
     db = client['twitter_scrape']
     collection = db['snscrape_2']
     collection.delete_many({})
     dataframe_json= json.loads(mongo_data.to_json(orient='records'))
     collection.insert_many(dataframe_json)
     st.success('file ready for download')

#user selection download option
download_option = st.radio("please select your desired format",('JSON','CSV'))
#to download in json format   
if download_option=='JSON':
       start = tweet_scrap(search_query,start_date,end_date,tweet_limit)
       data_frame = display_df(start)
       st.write("click the link to download the file")
       json_string = data_frame.to_json(indent=2)
       b64 =  base64.b64encode(json_string.encode()).decode()
       json_filename = search_query.replace(' ', '_') + '.json'
       href = f'<a href="data:file/json;base64,{b64}" download="{json_filename}">Download json File</a>'
       st.markdown(href, unsafe_allow_html=True)

#to download in csv format       
else:
       start = tweet_scrap(search_query,start_date,end_date,tweet_limit)
       data_frame = display_df(start)
       st.write("click the link to download the file")
       csv = data_frame.to_csv(index=False)
       b64 =  base64.b64encode(csv.encode()).decode()
       csv_filename = search_query.replace(' ', '_') + '.csv'
       href = f'<a href="data:file/csv;base64,{b64}" download="{csv_filename}">Download CSV File</a>'
       st.markdown(href, unsafe_allow_html=True)

with st.sidebar:
    st.subheader("Choose to have a graphical insights")
    option_name = ["most liked tweets", "most retweeted tweets"]
    option = st.radio("View in graph",option_name)
    chart= st.button('Get Chart')

#plotly part to disply data in bar chart 
if chart:    
   if option == "most liked tweets":
        tweet_eda = tweet_scrap(search_query, start_date, end_date, tweet_limit)
        tweet_data = display_df(tweet_eda)
        st.write(fig, 'your chart is here')
        fig = px.bar(tweet_data, x="user", y="likes")
        
   else:
        tweet_retweet = tweet_scrap(search_query, start_date, end_date, tweet_limit)
        tweet_data = display_df(tweet_retweet)
        st.write(figure, 'your graph is here')
        figure = px.bar(tweet_data, x="user", y="retweets")

#run streamlit in background
!streamlit run /content/streamlit.py &>/content/logs.txt &

!npx localtunnel --port 8501

